{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a95b1a-34af-40d5-ad27-0675d8f53f7e",
   "metadata": {},
   "source": [
    "## MACHINE LEARNING DAY - 16 : Accuracy Measures in Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432b68b-70df-4a22-98e9-74c6fbad3799",
   "metadata": {},
   "source": [
    "### What is a Confusion Matrix?\n",
    "\n",
    "A **confusion matrix** is a performance measurement tool for classification problems. It is a **table** that shows the actual vs. predicted classifications, and helps us understand how well a classification model is performing.\n",
    "\n",
    "---\n",
    "\n",
    "### Structure of a Confusion Matrix (for Binary Classification)\n",
    "\n",
    "|                      | **Predicted: Positive** | **Predicted: Negative** |\n",
    "| -------------------- | ----------------------- | ----------------------- |\n",
    "| **Actual: Positive** | True Positive (TP)      | False Negative (FN)     |\n",
    "| **Actual: Negative** | False Positive (FP)     | True Negative (TN)      |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Accuracy Measures (Metrics)\n",
    "\n",
    "1. **Accuracy**:\n",
    "\n",
    "   * The overall correctness of the model.\n",
    "   * **Formula**:\n",
    "\n",
    "     $$\n",
    "     \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "     $$\n",
    "\n",
    "2. **Precision** (also called Positive Predictive Value):\n",
    "\n",
    "   * Out of all predicted positives, how many were actually positive?\n",
    "   * **Formula**:\n",
    "\n",
    "     $$\n",
    "     \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "     $$\n",
    "\n",
    "3. **Recall** (also called Sensitivity or True Positive Rate):\n",
    "\n",
    "   * Out of all actual positives, how many did we correctly predict?\n",
    "   * **Formula**:\n",
    "\n",
    "     $$\n",
    "     \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "     $$\n",
    "\n",
    "4. **F1 Score**:\n",
    "\n",
    "   * Harmonic mean of Precision and Recall. Useful when you need a balance between Precision and Recall.\n",
    "   * **Formula**:\n",
    "\n",
    "     $$\n",
    "     \\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "     $$\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "\n",
    "Imagine a model that predicts whether an email is spam or not:\n",
    "\n",
    "* **TP (Spam correctly predicted as spam)** = 80\n",
    "* **FP (Not spam incorrectly predicted as spam)** = 10\n",
    "* **FN (Spam incorrectly predicted as not spam)** = 5\n",
    "* **TN (Not spam correctly predicted as not spam)** = 105\n",
    "\n",
    "Then:\n",
    "\n",
    "* Accuracy = (80 + 105) / (80 + 105 + 10 + 5) = **92.5%**\n",
    "* Precision = 80 / (80 + 10) = **88.9%**\n",
    "* Recall = 80 / (80 + 5) = **94.1%**\n",
    "* F1 Score â‰ˆ **91.4%**\n",
    "* Specificity = 105 / (105 + 10) = **91.3%**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
