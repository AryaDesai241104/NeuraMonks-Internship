{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "758625c9-6ce1-46b3-b828-0ce8cafbb01e",
   "metadata": {},
   "source": [
    "## MACHINE LEARNING DAY 24 : Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e3f4c-7430-4f26-9afa-10e5880aae37",
   "metadata": {},
   "source": [
    "### What is Dimensionality Reduction?\n",
    "\n",
    "Dimensionality reduction is a **technique used in machine learning and data preprocessing** to reduce the number of features (also known as variables, attributes, or dimensions) in a dataset, while **preserving the important patterns or structures**.\n",
    "\n",
    "High-dimensional datasets (i.e., datasets with many features) can be complex, computationally expensive, and prone to overfitting. Dimensionality reduction helps address these challenges by simplifying the dataset, which often leads to:\n",
    "\n",
    "* Reduced model complexity and training time\n",
    "* Improved model performance\n",
    "* Enhanced visualization and interpretability\n",
    "* Less storage and memory usage\n",
    "* Better handling of multicollinearity and noise\n",
    "\n",
    "---\n",
    "\n",
    "### Why is Dimensionality Reduction Needed?\n",
    "\n",
    "1. **Curse of Dimensionality**: As the number of features increases, the data becomes sparse, making it harder for models to find patterns.\n",
    "2. **Overfitting**: Too many features increase the risk that the model fits noise rather than signal.\n",
    "3. **Computation**: High-dimensional data requires more memory, processing time, and power.\n",
    "4. **Data Visualization**: It is hard to visualize datasets with more than 3 dimensions. Dimensionality reduction enables us to project high-dimensional data into 2D or 3D for better understanding.\n",
    "5. **Noise Reduction**: Many features might be irrelevant or redundant. Removing them reduces noise.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Dimensionality Reduction\n",
    "\n",
    "There are **two major types** of dimensionality reduction techniques:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Feature Selection**\n",
    "\n",
    "Feature selection **selects a subset of relevant original features** from the dataset without altering them. It does not create new features but filters out those that are not useful for the model.\n",
    "\n",
    "#### Key Characteristics:\n",
    "\n",
    "* Keeps original features intact.\n",
    "* Based on statistical tests or model performance.\n",
    "* Easier to interpret since original meaning of variables is retained.\n",
    "\n",
    "#### Methods of Feature Selection:\n",
    "\n",
    "**a. Filter Methods** – Use statistical measures to score features without involving any machine learning algorithm:\n",
    "\n",
    "* **Correlation Coefficient**: Select features that are highly correlated with the target and not highly correlated with each other.\n",
    "* **Chi-Square Test**: Used when both features and targets are categorical.\n",
    "* **ANOVA (Analysis of Variance)**: Used when features are numerical and the target is categorical.\n",
    "\n",
    "**b. Wrapper Methods** – Use a machine learning model to evaluate feature subsets:\n",
    "\n",
    "* **Recursive Feature Elimination (RFE)**: Recursively removes the least important features.\n",
    "* **Forward Selection**: Starts with zero features and adds one at a time.\n",
    "* **Backward Elimination**: Starts with all features and removes the least important ones.\n",
    "\n",
    "**c. Embedded Methods** – Perform feature selection as part of model training:\n",
    "\n",
    "* **Lasso Regression (L1 Regularization)**: Shrinks some coefficients to zero, effectively removing features.\n",
    "* **Tree-based Methods (e.g., Random Forest)**: Provide feature importance scores to rank and select variables.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Feature Extraction**\n",
    "\n",
    "Feature extraction **transforms the data** from the high-dimensional space to a lower-dimensional space by **creating new features** that are combinations of the original ones.\n",
    "\n",
    "#### Key Characteristics:\n",
    "\n",
    "* New features may not have direct interpretations.\n",
    "* Preserves most of the data’s structure or variance.\n",
    "* Especially useful when feature relationships are complex or nonlinear.\n",
    "\n",
    "#### Common Feature Extraction Methods:\n",
    "\n",
    "**a. Principal Component Analysis (PCA)**\n",
    "\n",
    "* Unsupervised linear transformation technique.\n",
    "* Projects data onto orthogonal axes (principal components) that capture maximum variance.\n",
    "* The first few components usually capture most of the variability in the data.\n",
    "\n",
    "**Steps in PCA**:\n",
    "\n",
    "1. Standardize the data\n",
    "2. Compute the covariance matrix\n",
    "3. Calculate eigenvalues and eigenvectors\n",
    "4. Choose top k eigenvectors (components)\n",
    "5. Project the data onto the new subspace\n",
    "\n",
    "**Use Case**: Data compression, noise reduction, visualization.\n",
    "\n",
    "---\n",
    "\n",
    "**b. Linear Discriminant Analysis (LDA)**\n",
    "\n",
    "* Supervised technique.\n",
    "* Maximizes class separability by finding axes that maximize **between-class variance** and minimize **within-class variance**.\n",
    "* Works well when the classes are linearly separable.\n",
    "\n",
    "**Use Case**: Classification problems with labeled data.\n",
    "\n",
    "---\n",
    "\n",
    "**c. t-SNE (t-distributed Stochastic Neighbor Embedding)**\n",
    "\n",
    "* Non-linear dimensionality reduction for **visualization**.\n",
    "* Converts high-dimensional similarities into low-dimensional space while preserving local structure.\n",
    "\n",
    "**Use Case**: Visualizing complex data in 2D or 3D (e.g., word embeddings, image clusters).\n",
    "\n",
    "---\n",
    "\n",
    "**d. UMAP (Uniform Manifold Approximation and Projection)**\n",
    "\n",
    "* Like t-SNE but faster and scalable.\n",
    "* Preserves both **local and global structure** of data.\n",
    "* Useful for visualization and preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "**e. Autoencoders**\n",
    "\n",
    "* Neural networks trained to reconstruct their input.\n",
    "* The bottleneck layer (compressed representation) serves as a lower-dimensional encoding.\n",
    "* Can capture nonlinear feature relationships.\n",
    "\n",
    "**Use Case**: Deep learning-based dimensionality reduction, especially for image, audio, or time-series data.\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Selection vs. Feature Extraction – Summary\n",
    "\n",
    "| Aspect                | Feature Selection            | Feature Extraction                    |\n",
    "| --------------------- | ---------------------------- | ------------------------------------- |\n",
    "| Output                | Subset of original features  | New features created from originals   |\n",
    "| Interpretability      | High                         | Often low                             |\n",
    "| Complexity            | Simpler                      | Often more complex (e.g., PCA, NN)    |\n",
    "| Information preserved | May lose some                | Tries to preserve maximum information |\n",
    "| Examples              | Correlation, Chi-Square, RFE | PCA, LDA, t-SNE, Autoencoders         |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
